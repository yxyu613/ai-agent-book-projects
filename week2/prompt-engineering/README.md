# Prompt Engineering Ablation Study for Tau-Bench

## æ¦‚è¿° (Overview)

è¿™ä¸ªé¡¹ç›®æ‰©å±•äº† Tau-Bench æ¡†æ¶ï¼Œæ·»åŠ äº†ä¸‰ä¸ªå…³é”®çš„æ¶ˆèç ”ç©¶é€‰é¡¹ï¼Œç”¨äºæ¼”ç¤º**æç¤ºå·¥ç¨‹ï¼šæŠŠ Agent çœ‹æˆèªæ˜çš„æ–°å‘˜å·¥**ç« èŠ‚çš„é‡è¦æ€§ã€‚é€šè¿‡è¿™äº›å®éªŒï¼Œæˆ‘ä»¬å¯ä»¥é‡åŒ–ä¸åŒæç¤ºå·¥ç¨‹å› ç´ å¯¹ Agent æ€§èƒ½çš„å½±å“ã€‚

This project extends the Tau-Bench framework with three critical ablation study options to demonstrate the importance of prompt engineering - treating agents as smart new employees. Through these experiments, we can quantify the impact of different prompt engineering factors on agent performance.

## æ¶ˆèç ”ç©¶é€‰é¡¹ (Ablation Study Options)

### 1. è¯­æ°”é£æ ¼ (Tone Style) ğŸ­

æ¼”ç¤ºä¸åŒçš„æ²Ÿé€šé£æ ¼å¦‚ä½•å½±å“ Agent çš„è¡¨ç°ï¼š

- **default**: æ ‡å‡†ä¸“ä¸šè¯­æ°”ï¼ˆåŸºçº¿ï¼‰
- **trump**: Donald Trump é£æ ¼ - ä½¿ç”¨å¤¸å¼ çš„è¯­è¨€ã€é‡å¤å¼ºè°ƒã€è‡ªä¿¡çš„è¡¨è¿°
- **casual**: ä¼‘é—²å‹å¥½é£æ ¼ - å¤§é‡ä½¿ç”¨è¡¨æƒ…ç¬¦å·ã€ä¿šè¯­å’Œè½»æ¾çš„è¯­è¨€

**åŸç†**: è¯­æ°”ä¼šå½±å“ Agent çš„ä¸“ä¸šæ€§å’Œä»»åŠ¡å®Œæˆè´¨é‡ã€‚è¿‡äºéšæ„æˆ–å¤¸å¼ çš„è¯­æ°”å¯èƒ½å¯¼è‡´ï¼š
- é™ä½ç”¨æˆ·ä¿¡ä»»åº¦
- å¢åŠ è¯¯è§£çš„å¯èƒ½æ€§
- å½±å“ä»»åŠ¡æ‰§è¡Œçš„å‡†ç¡®æ€§

### 2. Wiki è§„åˆ™éšæœºåŒ– (Wiki Rule Randomization) ğŸ“

ä½¿ç”¨é¢„ç”Ÿæˆçš„æåº¦æ··ä¹±ç‰ˆæœ¬çš„ wiki.mdï¼š

- ç§»é™¤æ‰€æœ‰ç« èŠ‚æ ‡é¢˜å’Œç»“æ„
- æ¯æ¡è§„åˆ™åŠ ä¸Šæ“ä½œä¸Šä¸‹æ–‡å‰ç¼€ï¼ˆå¦‚"When booking flights"ï¼‰
- å°†æ‰€æœ‰è§„åˆ™å®Œå…¨æ‰“ä¹±æˆä¸€ä¸ªå¹³é¢åˆ—è¡¨
- ç ´åè§„åˆ™ä¹‹é—´çš„é€»è¾‘å…³ç³»

**åŸç†**: è‰¯å¥½ç»„ç»‡çš„æŒ‡ä»¤å°±åƒç»™æ–°å‘˜å·¥çš„åŸ¹è®­æ‰‹å†Œã€‚æåº¦éšæœºåŒ–ä¼šï¼š
- å®Œå…¨ç ´åä¿¡æ¯çš„é€»è¾‘å±‚çº§
- æ··æ·†ä¸åŒæ“ä½œçš„è§„åˆ™ç•Œé™
- ä½¿ Agent æéš¾ç†è§£ä»»åŠ¡ä¼˜å…ˆçº§å’Œè§„åˆ™å…³è”
- å¢åŠ è¯¯ç”¨è§„åˆ™å’Œé—æ¼å…³é”®æ­¥éª¤çš„é£é™©

### 3. å·¥å…·æè¿°ç§»é™¤ (Tool Description Removal) ğŸ”§

ç§»é™¤æ‰€æœ‰å·¥å…·å’Œå‚æ•°çš„æè¿°ä¿¡æ¯ï¼š

- å·¥å…·å‡½æ•°æè¿°è®¾ä¸ºç©ºå­—ç¬¦ä¸²
- å‚æ•°æè¿°è®¾ä¸ºç©ºå­—ç¬¦ä¸²
- æµ‹è¯•æ˜ç¡®è¯´æ˜çš„é‡è¦æ€§

**åŸç†**: æ¸…æ™°çš„å·¥å…·æè¿°å°±åƒå‘˜å·¥æ‰‹å†Œä¸­çš„æ“ä½œæŒ‡å—ã€‚ç§»é™¤æè¿°ä¼šï¼š
- Agent ä¸ç†è§£å·¥å…·çš„ç”¨é€”
- å¢åŠ é”™è¯¯ä½¿ç”¨å·¥å…·çš„æ¦‚ç‡
- é™ä½ä»»åŠ¡å®Œæˆç‡

## å®‰è£… (Installation)

é¦–å…ˆç¡®ä¿å·²å®‰è£…åŸºç¡€ Tau-Bench ä¾èµ–ï¼š

```bash
cd projects/week2/prompt-engineering
pip install -r requirements.txt
```

## ä½¿ç”¨æ–¹æ³• (Usage)

### åŸºç¡€è¿è¡Œ (Basic Run)

è¿è¡ŒåŸºçº¿å®éªŒï¼ˆæ— æ¶ˆèï¼‰ï¼š

```bash
python run_ablation.py \
    --model openai/gpt-5 \
    --env airline \
    --task-split test \
    --start-index 0 \
    --end-index 10
# Note: Provider will be automatically set to 'openrouter' for openai/gpt-5
```

### è¯­æ°”æ¶ˆèå®éªŒ (Tone Ablation)

#### Trump é£æ ¼
```bash
python run_ablation.py \
    --model openai/gpt-5 \
    --env airline \
    --tone-style trump \
    --ablation-name trump_tone
```

#### ä¼‘é—²é£æ ¼
```bash
python run_ablation.py \
    --model openai/gpt-5 \
    --env airline \
    --tone-style casual \
    --ablation-name casual_tone
```

### Wiki éšæœºåŒ–å®éªŒ (Wiki Randomization)

```bash
python run_ablation.py \
    --model openai/gpt-5 \
    --env airline \
    --randomize-wiki \
    --ablation-name wiki_random
```

### å·¥å…·æè¿°ç§»é™¤å®éªŒ (Tool Description Removal)

```bash
python run_ablation.py \
    --model openai/gpt-5 \
    --env airline \
    --remove-tool-descriptions \
    --ablation-name no_tool_desc
```

### ç»„åˆæ¶ˆèå®éªŒ (Combined Ablations)

æµ‹è¯•å¤šä¸ªå› ç´ çš„ç»„åˆå½±å“ï¼š

```bash
python run_ablation.py \
    --model openai/gpt-5 \
    --env airline \
    --tone-style casual \
    --randomize-wiki \
    --remove-tool-descriptions \
    --ablation-name full_ablation
```

## å®éªŒè„šæœ¬ (Experiment Scripts)

### è¿è¡Œå®Œæ•´æ¶ˆèç ”ç©¶

åˆ›å»º `run_full_ablation.sh`:

```bash
#!/bin/bash

MODEL="openai/gpt-5"
# Provider will be auto-detected (openrouter for openai/gpt-5)
ENV="airline"

# Baseline
echo "Running baseline..."
python run_ablation.py --model $MODEL --env $ENV --ablation-name baseline

# Tone variations
echo "Running tone ablations..."
python run_ablation.py --model $MODEL --env $ENV --tone-style trump --ablation-name tone_trump
python run_ablation.py --model $MODEL --env $ENV --tone-style casual --ablation-name tone_casual

# Wiki randomization
echo "Running wiki randomization..."
python run_ablation.py --model $MODEL --env $ENV --randomize-wiki --ablation-name wiki_random

# Tool description removal
echo "Running tool description removal..."
python run_ablation.py --model $MODEL --env $ENV --remove-tool-descriptions --ablation-name no_tools

# Worst case - all ablations
echo "Running worst case scenario..."
python run_ablation.py --model $MODEL --env $ENV \
    --tone-style casual --randomize-wiki --remove-tool-descriptions --ablation-name worst_case

echo "All experiments complete!"
```

## ç»“æœåˆ†æ (Result Analysis)

ç»“æœä¿å­˜åœ¨ `results_ablation/` ç›®å½•ï¼ŒåŒ…å«ï¼š

- **task_id**: ä»»åŠ¡æ ‡è¯†
- **reward**: æˆåŠŸç‡ï¼ˆ0 æˆ– 1ï¼‰
- **info**: è¯¦ç»†æ‰§è¡Œä¿¡æ¯
- **traj**: å®Œæ•´å¯¹è¯è½¨è¿¹
- **ablation_config**: ä½¿ç”¨çš„æ¶ˆèé…ç½®

## é¢„æœŸç»“æœ (Expected Results)

åŸºäºæç¤ºå·¥ç¨‹åŸç†ï¼Œé¢„æœŸæ€§èƒ½æ’åºï¼š

1. **Baseline**: æ ‡å‡†é…ç½®ï¼Œæœ€ä½³æ€§èƒ½
2. **Tone variations**: è¯­æ°”å˜åŒ–ä¸ä¼šæ˜¾è‘—å½±å“äº¤äº’è´¨é‡
3. **Wiki randomization**: æåº¦æ··ä¹±çš„è§„åˆ™æ’åˆ—ä¸¥é‡å½±å“ç†è§£ï¼Œå®¹æ˜“å‡ºç°æŒ‡ä»¤ä¸éµå¾ªé—®é¢˜
4. **No tool descriptions**: ç¼ºä¹å·¥å…·è¯´æ˜å¯¼è‡´å¤§é‡å·¥å…·è°ƒç”¨å‚æ•°é”™è¯¯ï¼Œå¯¼è‡´æ‰§è¡Œé”™è¯¯çš„æ“ä½œ
5. **Combined ablations**: å¤šé‡å› ç´ å åŠ ï¼Œæ€§èƒ½æœ€å·®

## å…³é”®æ´å¯Ÿ (Key Insights)

è¿™äº›å®éªŒæ¼”ç¤ºäº†ä¸ºä»€ä¹ˆè¦**æŠŠ Agent çœ‹æˆèªæ˜çš„æ–°å‘˜å·¥**ï¼š

### 1. æ¸…æ™°çš„æŒ‡ä»¤è‡³å…³é‡è¦
å°±åƒåŸ¹è®­æ–°å‘˜å·¥ï¼ŒAgent éœ€è¦ï¼š
- ç»“æ„åŒ–çš„ä¿¡æ¯
- æ¸…æ™°çš„ä»»åŠ¡æè¿°
- æ˜ç¡®çš„å·¥å…·ä½¿ç”¨è¯´æ˜

### 2. ä¸Šä¸‹æ–‡ç»„ç»‡å½±å“ç†è§£
- é€»è¾‘æ’åºçš„è§„åˆ™æ›´å®¹æ˜“éµå¾ª
- ç›¸å…³ä¿¡æ¯åº”è¯¥ç»„åˆåœ¨ä¸€èµ·
- ä¼˜å…ˆçº§åº”è¯¥æ˜ç¡®

### 3. å·¥å…·æ–‡æ¡£ä¸å¯æˆ–ç¼º
- æ¯ä¸ªå·¥å…·éœ€è¦æ¸…æ™°çš„ç”¨é€”è¯´æ˜
- å‚æ•°æè¿°é˜²æ­¢è¯¯ç”¨
- ç¤ºä¾‹æœ‰åŠ©äºæ­£ç¡®ä½¿ç”¨

## å‚æ•°è¯´æ˜ (Parameters)

| å‚æ•° | è¯´æ˜ | é€‰é¡¹ |
|------|------|------|
| `--tone-style` | è¯­æ°”é£æ ¼ï¼ˆåº”ç”¨åˆ°ç³»ç»Ÿæç¤ºï¼‰ | default, trump, casual |
| `--randomize-wiki` | éšæœºåŒ–wikiè§„åˆ™ | flag |
| `--remove-tool-descriptions` | ç§»é™¤å·¥å…·æè¿° | flag |
| `--ablation-name` | å®éªŒåç§°æ ‡è¯† | string |
| `--env` | ç¯å¢ƒé€‰æ‹© | airline, retail |
| `--model` | ä½¿ç”¨çš„æ¨¡å‹ | string (e.g., openai/gpt-5) |
| `--model-provider` | æ¨¡å‹æä¾›å•†ï¼ˆå¯é€‰ï¼‰ | è‡ªåŠ¨æ£€æµ‹ï¼ˆopenai/gpt-5ä½¿ç”¨openrouterï¼‰ |
| `--task-split` | ä»»åŠ¡é›† | train, test, dev |
| `--start-index` | èµ·å§‹ä»»åŠ¡ç´¢å¼• | integer |
| `--end-index` | ç»“æŸä»»åŠ¡ç´¢å¼• | integer |
| `--log-dir` | ç»“æœä¿å­˜ç›®å½• | string |

## æ•…éšœæ’é™¤ (Troubleshooting)

### å¸¸è§é—®é¢˜

1. **ImportError**: ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•è¿è¡Œå¹¶å®‰è£…æ‰€æœ‰ä¾èµ–
2. **APIé”™è¯¯**: æ£€æŸ¥APIå¯†é’¥è®¾ç½®å’Œé…é¢
3. **å†…å­˜é—®é¢˜**: å‡å°‘ `--max-concurrency` å‚æ•°

### è°ƒè¯•æ¨¡å¼

æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼š
```bash
export LITELLM_LOG=DEBUG
python run_ablation.py ...
```

## è´¡çŒ® (Contributing)

æ¬¢è¿è´¡çŒ®æ›´å¤šæ¶ˆèç ”ç©¶é€‰é¡¹ï¼è¯·è€ƒè™‘æ·»åŠ ï¼š
- ä¸åŒçš„è¯­æ°”é£æ ¼
- å…¶ä»–wikiç»„ç»‡æ–¹å¼
- æ›´å¤šå·¥å…·æè¿°å˜ä½“
- æ€§èƒ½å¯è§†åŒ–å·¥å…·

## æ€»ç»“ (Summary)

è¿™ä¸ªæ¶ˆèç ”ç©¶æ¡†æ¶é‡åŒ–å±•ç¤ºäº†è‰¯å¥½æç¤ºå·¥ç¨‹çš„é‡è¦æ€§ã€‚é€šè¿‡ç³»ç»Ÿåœ°é™è§£ä¸åŒæ–¹é¢çš„æç¤ºè´¨é‡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼š

- **30-80%çš„æ€§èƒ½ä¸‹é™**å½“æç¤ºå·¥ç¨‹ä¸å½“æ—¶
- **ç»“æ„å’Œæ¸…æ™°åº¦**æ˜¯æœ€å…³é”®çš„å› ç´ 
- **ä¸“ä¸šæ€§å’Œä¸€è‡´æ€§**å»ºç«‹æœ‰æ•ˆçš„Agentç³»ç»Ÿ

è®°ä½ï¼šä¼˜ç§€çš„æç¤ºå·¥ç¨‹å°±æ˜¯ä¼˜ç§€çš„å‘˜å·¥åŸ¹è®­ï¼

---

*æœ¬é¡¹ç›®æ˜¯ã€ŠAI Agent å®æˆ˜ã€‹ç¬¬2å‘¨"æç¤ºå·¥ç¨‹"ç« èŠ‚çš„é…å¥—ä»£ç ã€‚*

---

# Ï„-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains

**â—News**: We have released [Ï„Â²-bench](https://github.com/sierra-research/tau2-bench) as an extension of $\tau$-bench. $\tau^2$-bench includes code fixes and an additional `telecom` domain focusing on troubleshooting scenarios. Please use the $\tau^2$-bench as the latest version of this benchmark.

**Paper**:
* [Ï„-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains](https://arxiv.org/abs/2406.12045)
* [Ï„Â²-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)

We propose $\tau$-bench, a benchmark emulating dynamic conversations between a user (simulated by language models) and a language agent provided with domain-specific API tools and policy guidelines.

## Leaderboard

### Airline

| Strategy       | Pass^1 | Pass^2 | Pass^3 | Pass^4 |
| -------------- | ------ | ------ | ------ | ------ |
| [TC (claude-3-5-sonnet-20241022)](https://www.anthropic.com/news/3-5-models-and-computer-use)      | **0.460**     | **0.326**     | **0.263**     | **0.225**     |
| [TC (gpt-4o)](https://platform.openai.com/docs/guides/function-calling)     | 0.420     | 0.273     | 0.220     | 0.200     |
| [TC (claude-3-5-sonnet-20240620)](https://docs.anthropic.com/en/docs/build-with-claude/tool-use)      | 0.360     | 0.224     | 0.169     | 0.139     |
| [TC (mistral-large-2407)](https://docs.mistral.ai/capabilities/function_calling/)     | ??     | ??     | ??     | ??     |
| [TC (gpt-4o-mini)](https://platform.openai.com/docs/guides/function-calling)     | 0.225     | 0.140     | 0.110     | 0.100     |
| [Act](https://arxiv.org/abs/2210.03629) (gpt-4o)     | 0.365 | 0.217 | 0.160 | 0.140     |
| [ReAct](https://arxiv.org/abs/2210.03629) (gpt-4o)     | 0.325 | 0.233 | 0.185 | 0.160     |

### Retail

| Strategy       | Pass^1 | Pass^2 | Pass^3 | Pass^4 |
| -------------- | ------ | ------ | ------ | ------ |
| [TC (claude-3-5-sonnet-20241022)](https://www.anthropic.com/news/3-5-models-and-computer-use)      | **0.692**     | **0.576**     | **0.509**     | **0.462**     |
| [TC (gpt-4o)](https://platform.openai.com/docs/guides/function-calling)     | 0.604     | 0.491     | 0.430     | 0.383     |
| [TC (claude-3-5-sonnet-20240620)](https://docs.anthropic.com/en/docs/build-with-claude/tool-use)      | 0.626     | 0.506     | 0.435     | 0.387     |
| [TC (mistral-large-2407)](https://docs.mistral.ai/capabilities/function_calling/)     | ??     | ??     | ??     | ??     |
| [TC (gpt-4o-mini)](https://platform.openai.com/docs/guides/function-calling)     | ??     | ??     | ??     | ??     |
| [Act](https://arxiv.org/abs/2210.03629) (gpt-4o)     | ??     | ??     | ??     | ??     |
| [ReAct](https://arxiv.org/abs/2210.03629) (gpt-4o)     | ??     | ??     | ??     | ??     |

*TC = `tool-calling` strategy (the function-calling strategy reported in the paper)

## Setup

1. Clone this repository:

```bash
git clone https://github.com/sierra-research/tau-bench && cd ./tau-bench
```

2. Install from source (which also installs required packages):

```bash
pip install -e .
```

3. Set up your OpenAI / Anthropic / Google / Mistral / AnyScale API keys as environment variables.

```bash
OPENAI_API_KEY=...
ANTHROPIC_API_KEY=...
GOOGLE_API_KEY=...
MISTRAL_API_KEY=...
```

## Run

Run a tool-calling agent on the Ï„-retail environment:

```bash
python run.py --agent-strategy tool-calling --env retail --model gpt-4o --model-provider openai --user-model gpt-4o --user-model-provider openai --user-strategy llm --max-concurrency 10
```

Set max concurrency according to your API limit(s).

To run specific tasks, use the `--task-ids` flag. For example:

```bash
python run.py --agent-strategy tool-calling --env retail --model gpt-4o --model-provider openai --user-model gpt-4o --user-model-provider openai --user-strategy llm --max-concurrency 10 --task-ids 2 4 6
```

This command will run only the tasks with IDs 2, 4, and 6.

## User simulators

By default, we use `gpt-4o` as the user simulator with strategy `llm`. You can use other models by setting the `--user-model` flag, or other strategies by setting the `--user-strategy` flag. For example, run a tool-calling agent with a claude user simulator:

```bash
python run.py --agent-strategy tool-calling --env retail --model gpt-4o --model-provider openai --max-concurrency 10 --user-model claude-3-5-sonnet-20240620 --user-model-provider anthropic --user-strategy llm
```

Other strategies:

To run `react` user simulator:

```bash
python run.py --agent-strategy tool-calling --env retail --model gpt-4o --model-provider openai --max-concurrency 10 --user-model gpt-4o --user-model-provider openai --user-strategy react
```

Example of a `react` user response:

```md
Thought:
I should provide my name and zip code as I wasn't given an email address to use.

User Response:
Sure, my name is Yusuf Rossi, and my zip code is 19122.
```

To run `verify` user simulator:

```bash
python run.py --agent-strategy tool-calling --env retail --model gpt-4o --model-provider openai --max-concurrency 10 --user-model gpt-4o --user-model-provider openai --user-strategy verify
```

This strategy uses a subsequent LLM verification step to check if the user simulator's response is satisfactory. If not, the user simulator will be prompted to generate a new response.

To run `reflection` user simulator:

```bash
python run.py --agent-strategy tool-calling --env retail --model gpt-4o --model-provider openai --max-concurrency 10 --user-model gpt-4o --user-model-provider openai --user-strategy reflection
```

This strategy uses a subsequent LLM verification step to check if the user simulator's response is satisfactory. If not, the user simulator will be prompted to reflect on its response and generate a new response.

## Auto error identification

Often times, it is difficult and time consuming to manually identify specific error locations in trajectories as they can be long and the constraints can be complex. We have provided an auto error identification tool that can do the following:

1. Fault assignment: determine the entity that is responsible for the fault (user, agent, environment)
2. Fault type classification: classify the type of fault (goal_partially_completed, used_wrong_tool, used_wrong_tool_argument, took_unintended_action)

Both of the labels are accompanied with a description.

To run the auto error identification, run:

```bash
python auto_error_identification.py --env <airline/retail> --platform openai --results-path <the path to your results file here> --max-concurrency 16 --output-path test-auto-error-identification --max-num-failed-results 10
```

Please note that this feature utilizes an LLM, which may lead to inaccurate error identifications.

*Notice: If an error is raised due to the structure of your results file, you may have to rerun the benchmark to produce a new results file. We have recently [rewritten](https://github.com/sierra-research/tau-bench/commit/043b544371757ebb3762b3d02a6675dfe0c41798) the benchmark to be more type-safe and extensible.

## Historical trajectories

Ï„-bench might be expensive to run. We have provided a set of historical trajectories for the airline and retail environments in `./historical_trajectories`.

If you would like to contribute your historical trajectories to this benchmark, please submit a PR!

## License

See `./LICENSE`.

## Contact

Please submit issues or pull requests if you find problems with the benchmark.

## Citation

```bibtex
@misc{yao2024tau,
      title={$\tau$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains}, 
      author={Shunyu Yao and Noah Shinn and Pedram Razavi and Karthik Narasimhan},
      year={2024},
      eprint={2406.12045},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2406.12045}, 
}
@misc{barres2025tau2,
      title={$\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment}, 
      author={Victor Barres and Honghua Dong and Soham Ray and Xujie Si and Karthik Narasimhan},
      year={2025},
      eprint={2506.07982},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.07982}, 
}
```
