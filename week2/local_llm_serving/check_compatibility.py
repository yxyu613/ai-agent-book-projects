#!/usr/bin/env python3
"""
Check system compatibility for running vLLM tool calling demo
"""

import sys
import platform
import subprocess
import shutil


def check_system():
    """Check system compatibility"""
    print("="*60)
    print("üîç System Compatibility Check")
    print("="*60)
    
    # Get system info
    system = platform.system()
    machine = platform.machine()
    python_version = sys.version_info
    
    print(f"\nüìä System Information:")
    print(f"  OS: {system} ({platform.platform()})")
    print(f"  Architecture: {machine}")
    print(f"  Python: {python_version.major}.{python_version.minor}.{python_version.micro}")
    
    # Check for CUDA
    cuda_available = False
    gpu_info = None
    
    print(f"\nüéÆ GPU Check:")
    
    if system == "Darwin":  # macOS
        print("  ‚ùå macOS detected - No CUDA support available")
        print("  ‚ÑπÔ∏è  Macs use Metal (Apple Silicon) or AMD/Intel GPUs")
        return False, "darwin"
    
    # Check for NVIDIA GPU
    if shutil.which("nvidia-smi"):
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=name,memory.total", "--format=csv,noheader"],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                gpu_info = result.stdout.strip()
                print(f"  ‚úÖ NVIDIA GPU found: {gpu_info}")
                cuda_available = True
            else:
                print("  ‚ö†Ô∏è  nvidia-smi found but couldn't query GPU")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Error checking GPU: {e}")
    else:
        print("  ‚ùå No NVIDIA GPU detected (nvidia-smi not found)")
    
    # Check PyTorch CUDA
    print(f"\nüî• PyTorch CUDA Check:")
    try:
        import torch
        if torch.cuda.is_available():
            print(f"  ‚úÖ PyTorch CUDA is available")
            print(f"  CUDA version: {torch.version.cuda}")
            print(f"  Number of GPUs: {torch.cuda.device_count()}")
            if torch.cuda.device_count() > 0:
                print(f"  GPU 0: {torch.cuda.get_device_name(0)}")
        else:
            print("  ‚ùå PyTorch CUDA is not available")
            cuda_available = False
    except ImportError:
        print("  ‚ö†Ô∏è  PyTorch not installed")
    
    return cuda_available, system.lower()


def provide_recommendations(cuda_available, system):
    """Provide recommendations based on system"""
    
    print("\n" + "="*60)
    print("üí° Recommendations")
    print("="*60)
    
    if cuda_available:
        print("\n‚úÖ Your system supports vLLM!")
        print("\nNext steps:")
        print("1. Install requirements: pip install -r requirements.txt")
        print("2. Run the main script: python main.py")
        print("3. The script will automatically use vLLM")
        
    elif system == "darwin" or system.lower() == "darwin":  # macOS
        print("\nüçé You're on macOS - will use Ollama")
        print("\nüìã Setup steps:\n")
        
        print("1Ô∏è‚É£  Install Ollama:")
        print("   brew install ollama")
        print("   ollama serve  # Run in separate terminal\n")
        
        print("2Ô∏è‚É£  Install a model with tool support:")
        print("   ollama pull qwen3:0.6b  # Default model for this project\n")
        
        print("3Ô∏è‚É£  Run the main script:")
        print("   python main.py")
        print("   # Will automatically use Ollama")
        
    elif system.lower() == "windows":  # Windows
        if not cuda_available:
            print("\nü™ü You're on Windows without CUDA - will use Ollama")
            print("\nüìã Setup steps:\n")
            
            print("1Ô∏è‚É£  Install Ollama:")
            print("   Download from: https://ollama.com/download/windows")
            print("   Run OllamaSetup.exe\n")
            
            print("2Ô∏è‚É£  Install a model:")
            print("   ollama pull qwen3:0.6b  # Default model for this project\n")
            
            print("3Ô∏è‚É£  Run the main script:")
            print("   python main.py")
            print("   # Will automatically use Ollama")
        
    else:  # Linux without CUDA
        print("\nüêß You're on Linux without CUDA - will use Ollama")
        print("\nüìã Setup steps:\n")
        
        print("1Ô∏è‚É£  Install Ollama:")
        print("   curl -fsSL https://ollama.com/install.sh | sh")
        print("   systemctl start ollama  # Or: ollama serve\n")
        
        print("2Ô∏è‚É£  Install a model:")
        print("   ollama pull qwen3:0.6b  # Default model for this project\n")
        
        print("3Ô∏è‚É£  Run the main script:")
        print("   python main.py")
        print("   # Will automatically use Ollama")


def main():
    """Main compatibility check"""
    cuda_available, system = check_system()
    provide_recommendations(cuda_available, system)
    
    print("\n" + "="*60)
    print("For more details, see README.md")
    print("="*60)


if __name__ == "__main__":
    main()
